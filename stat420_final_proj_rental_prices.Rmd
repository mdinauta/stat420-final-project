---
title: "Using Craiglist Data to Explain Differences in Rental Prices"
author: 'Matthew DiNauta, Evgenia Resin, Calvin Kim'
output:
  html_document:
    df_print: paged
---



```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(faraway)
library(knitr)
library(lmtest)
library(MASS)
library(broom)
library(dplyr)

set.seed(20200805)
housing <- read.csv('housing.csv')
```

# Introduction

This dataset was accessed from the [Kaggle.com dataset repository](https://www.kaggle.com/austinreese/usa-housing-listings) on July 27th, 2020. It contains rental listing data scraped from the classifieds website [Craigslist.org](https://www.craigslist.org). 

After trimming, the data set includes 17 variables:

* price (Response Variable)  
* region  
* type (housing type: apartment, condo, house, etc)  
* sqft  
* beds  
* baths  
* cats_allowed  
* dogs_allowed  
* smoking_allowed  
* wheelchair_access  
* electric vehicle charge  
* comes furnished  
* laundry options  
* parking options  

The data set contains `r nrow(housing)` observations. This dataset is interesting due to the inclusion of variables representing a rental listing's "features", such as if pets are allowed, is smoking allowed, does it come furnished, etc, as well as variables that provide context about the listing, such as geographic location and square footage. With this dataset, we can explore questions such as:

* Controlling for geographic location and square footage, if a renter is considering getting a pet, how much (if at all) will this increase their expected rent? 

* Controlling for geographic location and square footage, if a landlord is considering adding on-premise laundry, how much (if at all) will they be able to increase rent?

The goal of the model will be understanding how these "features" of a rental property affect price. As such, we are less concerned with optimizing the predictive performance of the model than we are with making interpretable inferences. 

# Methods

## Data Cleaning

To prepare the data set for analysis, we will perform the following steps:

* We remove a few columns from the dataset that represent Craigslist metadata. These are:
    * ID
    * url
    * region_url
    * image_url
* We also remove the description data, which includes a text description of the property. This field may include interesting information, but would require parsing of the free-form text field into usable variables, and is beyond the scope of this analysis.  
* For similiar reasons, we will remove the lat/long variables  
* We cast the boolean variables as R factors.

We will also filter the dataset to a single region, Jacksonville FL, and remove the region feature. We decided to perform this step after exploring initial models which included `region` as a factor variable (for which there are `r length(levels(as.factor(housing$region)))` levels). This was difficult to manage, and the candidate models consistently violated the equal variance assumption of linear regression, a problem which we were not able to solve through transformations. 

However, we cannot ignore the effect of location. For example, if we wanted an estimate for how much on-site laundry may increase rent, it is important to remove the effect of location. Perhaps listings in rural areas are more likely to have on-site laundry than listings in major cities, such that it would appear that on-site laundry is associated with lower rental price. In this hypothetical example, after controlling for region, we may find that on-site laundry is in fact associated with higher rental price. 

So, our compromise is to remove the `region` factor, and simply focus the analysis on a single geographic region.

```{r}
housing_cleaned <- housing %>%
  filter(region == 'jacksonville',
         state == 'fl') %>%
  select(
    -c(
      id, 
      url, 
      region_url,
      image_url,
      lat,
      long,
      description)
    ) %>%
  mutate(
    type = as.factor(type),
    region = as.factor(region),
    state = as.factor(state),
    cats_allowed = as.factor(cats_allowed),
    dogs_allowed = as.factor(dogs_allowed),
    smoking_allowed = as.factor(smoking_allowed),
    wheelchair_access = as.factor(wheelchair_access),
    electric_vehicle_charge = as.factor(electric_vehicle_charge),
    comes_furnished = as.factor(comes_furnished),
    laundry_options = as.factor(laundry_options),
    parking_options = as.factor(parking_options))
```

Our cleaned dataset now has `r nrow(housing_cleaned)` rows.

## Exploratory analysis

Our dataset contains many categorical variables, which we will refer to as factors to align with the R programming language's terminology. We will begin by examining the category-level frequencies of these factors. 

```{r}
with(housing_cleaned, table(type))
with(housing_cleaned, table(cats_allowed))
with(housing_cleaned, table(dogs_allowed))
with(housing_cleaned, table(smoking_allowed))
with(housing_cleaned, table(wheelchair_access))
with(housing_cleaned, table(electric_vehicle_charge))
with(housing_cleaned, table(comes_furnished))
with(housing_cleaned, table(laundry_options))
with(housing_cleaned, table(parking_options))
```

We note that the number of observations for individual levels in a factor may be quite inbalanced. We do not see the need to make any additional data cleaning steps at this point however.

Next, we'll examine the numeric columms, of which there are four: the predictors `sqft`, `beds` and `baths`, and the response `price`.

```{r}
ggplot(housing_cleaned, aes(x=sqfeet)) + 
  geom_density() +
  xlab('Square feet') +
  ylab('Density') +
  ggtitle('Density plot of square footage')

summary(housing_cleaned$sqfeet)
```

Although beds and baths are numeric, they have a small number of unique values, so we'll simply count up the instances of each distinct value to see if there are any outliers we need to remove.

```{r}
with(housing_cleaned, table(beds))
with(housing_cleaned, table(baths))
```

It is safe to assume that the listing with 0 beds or baths are "missing data" or input errors, so we will remove those observations.

```{r message=FALSE, warning=FALSE}
housing_cleaned <- housing_cleaned %>%
  filter(beds > 0,
         baths > 0)

ggplot(housing_cleaned, aes(x=beds)) + geom_histogram() + ggtitle('Histogram of number of beds')
ggplot(housing_cleaned, aes(x=baths)) + geom_histogram() + ggtitle('Histogram of number of baths')
```

```{r}
ggplot(housing_cleaned, aes(x=price)) + geom_density()
summary(housing_cleaned$price)
```

The distribution of `price` is right-skewed - we'll likely explore transformations in the model fitting stage.

### Correlations

Now that we've concluded our univariate data analysis and data cleaning, we can begin exploratory analysis into correlations. This will give us an idea of what to expect from our models. Since so many of our predictors are categorical and our response is numeric, we'll make heavy use of boxplots.

```{r}
ggplot(housing_cleaned, aes(x=type, y=price)) + geom_boxplot() + ggtitle('Price v. Type')
ggplot(housing_cleaned, aes(x=cats_allowed, y=price)) + geom_boxplot() + ggtitle('Price v. Cats Allowed')
ggplot(housing_cleaned, aes(x=dogs_allowed, y=price)) + geom_boxplot() + ggtitle('Price v. Dogs Allowed')
ggplot(housing_cleaned, aes(x=smoking_allowed, y=price)) + geom_boxplot() + ggtitle('Price v. Smoking Allowed')
ggplot(housing_cleaned, aes(x=wheelchair_access, y=price)) + geom_boxplot() + ggtitle('Price v. Wheelchair Access')
ggplot(housing_cleaned, aes(x=electric_vehicle_charge, y=price)) + geom_boxplot() + ggtitle('Price v. Electric Vehicle Charge')
ggplot(housing_cleaned, aes(x=comes_furnished, y=price)) + geom_boxplot() + ggtitle('Price v. Comes Furnished')
ggplot(housing_cleaned, aes(x=laundry_options, y=price)) + geom_boxplot() + ggtitle('Price v. Laundry Options')
ggplot(housing_cleaned, aes(x=parking_options, y=price)) + geom_boxplot() + ggtitle('Price v. Parking Options')
```

```{r}
ggplot(housing_cleaned, aes(x=as.factor(beds), y=price)) + geom_boxplot() + ggtitle('Price v. Beds')
ggplot(housing_cleaned, aes(x=as.factor(baths), y=price)) + geom_boxplot() + ggtitle('Price v. Baths')
ggplot(housing_cleaned, aes(x=sqfeet, y=price)) + geom_point() + ggtitle('Square footage v. Price')
```

Judging by the bivariate correlations only (without "controlling" for any other predictors), we observe the following:

* Price generally increaes with number of beds  
* Price generally increases with number of baths    
* Price genereally increases with square footage  
* Price varies by property type, e.g. "condos" fetch higher rents than "apartments"  
* Prices appear to be higher for listings where pets are *not* allowed  
* Prices are lower on listings where smoking is allowed  
* Prices are not obviously different for listings with wheelchair access  
* Prices are considerably higher for listings with electric vechicle charge  
* Prices are higher for furnished vs. unfurnished listings  
* Price is higher when a washer/dryer is in-unit/there are in-unit hookups available

## Models - Control variables

As a reminder, our goal in this analysis is to estimate the mean change in expected rental price when a rental property has certain amenities (e.g. in-unit laundry) or allowances, such as dogs. To understand the association of an amnenity such as "in-unit laundry", it will be important to "control for" things like location, square footage, and beds/baths. 

We'll consider the following variables the "control" variables:  

* beds  
* baths  
* square footage  
* property type

As a reminder, geographical location is implicitly controlled for, since our data set includes only listings in Jacksonville, FL. 

We are less interested in the specific coefficients from these variables - we simply want to make sure we are adjusting for them when analyzing our variables of interest. 

```{r}
mod_control_variables <- lm(
  price ~ 
    sqfeet +
    beds + 
    baths +
    type,
  data = housing_cleaned)

summary(mod_control_variables)$coefficients
```

These coefficient estimates are mostly intuitive and expected. The one surprising result is that the coefficient for `beds` is negative - we can see that the direction of the association between `beds` and `price` flips from postivie to negative when we add `sqfeet` to the model.

```{r}
summary(lm(price ~ beds, data=housing_cleaned))$coefficients
summary(lm(price ~ beds + sqfeet, data=housing_cleaned))$coefficients
```

This could be caused by collinearity, which we check below.

```{r}
vif(mod_control_variables)
```

Although `sqfeet` and `beds` are somewhat correlated, the VIF values of ~3 are acceptable, so the flipping of the sign does not appear to be due to collinearity. The below plot shows what is actually happening.

```{r}
tmp_data_for_plot <- housing_cleaned %>%
  mutate(sq_feet_ntile = ntile(sqfeet, 5)) %>%
  select(sq_feet_ntile,
         sqfeet,
         price,
         beds)

ggplot(tmp_data_for_plot, aes(x=as.factor(beds), y=price)) +
  geom_boxplot() +
  facet_wrap(~as.factor(sq_feet_ntile)) +
  xlab('Number of beds') +
  ylab('Price') +
  ggtitle('Price vs. Number of beds, facetted by square foot quintile')
```

When we facet the price vs. beds by square foot quintile, we see that the number of beds and price are in fact somewhat negatively correlated *within each quintile*. In other words, adding bedrooms while holding square footage constant decreases the price. This is somewhat inuitive, in that the size of each indidivudal bedroom becomes smaller, which may devalue the property. 

## Models - variables of interest

Next we'll add our variables of interest, and confirm that the inclusion of these variables is an improvement over a model with only the "control" variables. In practice, we are asking if certain features/amnenities of a rental listing help explain price, as opposed to knowing only the basic property facts such as square footage, number of beds, etc.

We'll compare three models: 1) the "control" model, 2) a "full" model that includes all of the variables available (control + variables of interest), and 3) the result of applying a backwards stepwise procedure on the "full" model. Given that the our price response is clearly right-skewed, we will apply a log transformation to it.  

Since we are primarily interested in inference and not predictive performance, we'll fit the models on the entire data set and compare them with adjusted $R^2$. Adjusted $R^2$ adjusts for the number of predictors in the model, so it provides some protection against selecting spurious variables. 

```{r}
control_model = lm(
  log(price) ~ 
    sqfeet +
    beds + 
    baths +
    type,
  data = housing_cleaned)

full_model = lm(
  log(price) ~ 
    type + 
    sqfeet + 
    beds + 
    baths + 
    cats_allowed + 
    dogs_allowed + 
    smoking_allowed + 
    wheelchair_access + 
    electric_vehicle_charge + 
    comes_furnished + 
    laundry_options + 
    parking_options, 
  data = housing_cleaned)

selected_model = step(full_model, direction = "backward", trace=FALSE)

control_model_stats = c("Adj. R2" = summary(control_model)$adj.r.squared)
full_model_stats = c("Adj. R2" = summary(full_model)$adj.r.squared)
selected_model_stats = c("Adj. R2" = summary(selected_model)$adj.r.squared)

kable(
  data.frame("Control Model" = control_model_stats, 
             "Full Model" = full_model_stats, 
             "Selected Model" = selected_model_stats), 
  format = "markdown", 
  col.names = c("Control Model", "Full Model", "Selected Model"))
```

The adjusted $R^2$ values are about equal for the "full" and "selected" models, while the selected model uses fewer variables. So, we will move forwared with the "selected" model, and check it's assumptions. 

## Model assumptions check

We'll check for outliers first.

```{r}
housing_cleaned[cooks.distance(selected_model) > 4/length(housing_cleaned),]
```
None of the observations are considered influential.

Next we'll check model's linearity and constant variance assumptions.

```{r}
plot(fitted(selected_model), resid(selected_model), 
       col = "darkblue", pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lwd = 2)
```

```{r, message=FALSE, warning=FALSE}
bptest(selected_model)
```

Fitted vs. Residuals Plot looks reasonable but Breusch-Pagan Test rejects the null hypothesis of constant variance. Although the formal test rejects the null hypothesis, we are working with a relatively large number of observations and we are satisfied that the assumptions hold given the visualizations. Our interpretation is that the null is rejected for a relatively minor deviation, due to our large sample size. 

Next we'll check normality assumption.

```{r}
qqnorm(resid(selected_model))
qqline(resid(selected_model))
```

```{r}
shapiro.test(resid(selected_model))
```

Q-Q plot has some tails but looks acceptable. Shapiro test, on the other hand, rejects Normality.
Again, like most statistical significance tests, if the sample size is sufficiently large this test may detect even trivial departures from the null hypothesis. Same as above, we trust Q-Q plot and are satisfied with normality assumtion. That said, we'll consider whether a box cox transformation is an improvement over the log transformation. 

```{r}
full_model_no_trans = lm(
  price ~ 
    type + 
    sqfeet + 
    beds + 
    baths + 
    cats_allowed + 
    dogs_allowed + 
    smoking_allowed + 
    wheelchair_access + 
    electric_vehicle_charge + 
    comes_furnished + 
    laundry_options + 
    parking_options, 
  data = housing_cleaned)

boxcox(full_model_no_trans, plotit = TRUE)
```

```{r}
full_model_boxcox_trans = lm(
  (price^-0.6 - 1) / - 0.6 ~ 
    type + 
    sqfeet + 
    beds + 
    baths + 
    cats_allowed + 
    dogs_allowed + 
    smoking_allowed + 
    wheelchair_access + 
    electric_vehicle_charge + 
    comes_furnished + 
    laundry_options + 
    parking_options, 
  data = housing_cleaned)

selected_model_boxcox_trans = step(full_model_boxcox_trans, direction = "backward", trace=FALSE)

full_mopdel_log = lm(
  log(price) ~ 
    type + 
    sqfeet + 
    beds + 
    baths + 
    cats_allowed + 
    dogs_allowed + 
    smoking_allowed + 
    wheelchair_access + 
    electric_vehicle_charge + 
    comes_furnished + 
    laundry_options + 
    parking_options, 
  data = housing_cleaned)

qqnorm(resid(selected_model_boxcox_trans))
qqline(resid(selected_model_boxcox_trans))
```

We can see that boxcox transformation appears to have produced a model with slightly better normality. However, we'll continue with log response transformation model because the difference is small and the simpler transformation will make interpreting our coefficients easier.

# Results

Below, we will print the model summary output, as well as plot the coefficient estimates in descending order. 

```{r}
summary(selected_model)
```

```{r}
library(broom)

coef_plot_data <- tidy(selected_model) %>%
  mutate(
    ymin = estimate - (qnorm(1 - 0.05 / 2) * std.error),
    ymax = estimate + (qnorm(1 - 0.05 / 2) * std.error)) %>%
  filter(term != '(Intercept)')

coef_plot_data$term <- factor(
  coef_plot_data$term, 
  levels = coef_plot_data$term[order(coef_plot_data$estimate)])

ggplot(coef_plot_data, aes(x=term, y=estimate)) + 
  geom_hline(yintercept=0) +  
  geom_pointrange(aes(ymin=ymin, ymax=ymax)) +
  labs(x="Coefficient", y="Estimate", title="Coefficients from selected model") + 
  coord_flip()
```

# Discussion

We have established that certain "features/amnenties" of a rental listing are significantly associated with price. Knowing this information helps us predict/explain price, beyond knowing the basics such as square footage, number of beds/baths, etc.

This has numerous practical implications. One is that a property owner/landlord can increase the value of their rental listing (i.e. raise rent) by improvements such as in-unit laundry, improving parking options through a garage or carport etc. They can use the coefficient estimates in a cost/benefit analysis. 

From a renters perspective, they can estimate how much additional rent they would expect to pay if requiring (for example) a furnished rental. 

The four feature/amnenity variables that are not included in the final model are `dogs_allowed`, `smoking_allowed`, `wheelchair_access`, and `electric_vehicle_charge`. Based on our earlier exploratory analysis, we suspect `dogs_allowed` was not selected due to being fairly collinear with `cats_allowed`, and `electric_vehicle_charge` was not selected due to having only a handful of observations for listings with electric vehicle charges.

As a reminder, we fit our model to the log of price. To interpret the point estimates, we will apply a transformation such that the coefficients can be interpreted as "a one-unit increase in x is associated with a n-*percent* change in mean price".

```{r}
transformed_coef <- (exp(coef(summary(selected_model))[,1]) -1) * 100
transformed_coef <- transformed_coef[2:length(transformed_coef)]
kable(transformed_coef, format = "markdown", col.names = c("Value"))
```

```{r}
kable(sort(abs(transformed_coef),decreasing = TRUE), format = "markdown", col.names = c("Absolute Value"))
```

Exploring the coefficients of the selected model, we see the correlation of variables to housing price. The two tables above show the transformed coefficients and the absolute values of the transformed coefficients so we can easily see which variables have the most impact on price. We see that among factor variables, `type` was among the most impactful variables, but the values seem unintuitive. Initially, we might expect houses to be more expensive than lofts, but our model shows lofts are associated the largest increase in mean price with a value of `r round(transformed_coef[[4]],3)` percent while the `type` value of house is associated with a decrease in mean price with a coefficient value of `r round(transformed_coef[[3]],3)` percent. Looking back at the data, we see that there were only two lofts in our data set while there were many more houses. Our hypothesis here is that lofts are generally located in more pricey neighborhoods. This is a situation where our model would benefit from more granular location data.

The furnished factor was the second most impactful factor variable with a transformed coefficient value of `r round(transformed_coef[[11]],2)`. Expectedly, a unit that is furnished would be more expensive than one that is not. The type of parking also falls in line with expectations with an attached or detatched garage having coefficient values of `r round(transformed_coef[[17]],2)` and `r round(transformed_coef[[19]],2)` while having only street parking available brought down prices with a coefficient value of `r round(transformed_coef[[22]],2)`. 

Laundry options also generally follow expectations with an in-unit laundry having a positive coefficient of `r round(transformed_coef[[16]],2)` while having laundry facilities on site or in the building had negative coefficients of `r round(transformed_coef[[13]],2)` and `r round(transformed_coef[[12]],2)`.
